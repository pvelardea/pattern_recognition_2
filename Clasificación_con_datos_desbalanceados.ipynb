{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación con datos desbalanceados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estrategias para resolver desequilibrio de datos en Python con la librería [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/).\n",
    "\n",
    "### Descripción de los datos\n",
    "El dataset contiene las transacciones de tarjeta de crédito de dos días realizadas en septiembre de 2013 por tarjetahabientes europeos. El dataset es altamente desbalanceado con un bajo porcentaje de transacciones fraudulentas entre numerosos registros de transacciones normales. La clase positiva (fraudes) representa el 0.172% (492 fraudes de 284,807 transacciones) del total de transacciones.\n",
    "\n",
    "Los rasgos `V1`, `V2`, .... `V28` son los componentes principales (PC) obtenidos con PCA, los únicos rasgos que no han sido transformados con PCA son `Time` y `Amount`. El rasgo `Time` contiene la estampa de tiempo en segundos de cada transacción a partir de la primera transacción. El rasgo `Class` es la variable objetivo con valor 1 en caso de fraude y 0 en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requerimientos\n",
    "Instalar la librería de Imbalanced Learn:<br>\n",
    "`pip install -U imbalanced-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO IGNORE DEPRECATION WARNINGS ###\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Cargar dataset\n",
    "Crear un dataframe para almacenar el dataset.<br>\n",
    "El dataset se encuentra disponible en: https://www.kaggle.com/mlg-ulb/creditcardfraud/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\creditcard.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explorar el dataframe\n",
    "Visualizar 5 muestras aleatorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificar los nombres de todas las columnas del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Limpieza de datos\n",
    "Los datasets pueden tener valores que no aparecen por una serie de razones, como observaciones que no se registraron y corrupción de datos. [Ref1](https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b), [Ref2](https://machinelearningmastery.com/handle-missing-data-python/)\n",
    "### 3.1 Detectar datos faltantes en el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contar valores núlos por rasgo\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para un chequeo rápido de todo el dataframe\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Desbalance de clases\n",
    "Ver número de filas y muestras por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(pd.value_counts(df['Class']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar desbalance mediante gráfica de barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(df['Class'])\n",
    "count_classes.plot(kind = 'bar',rot=0)\n",
    "LABELS = ['Normal','Fraude']\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.title(\"Frecuencia por número de observación\")\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Número of observaciones\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "De la gráfica anterior se puede observar que los datos están sesgados hacia la clase normal, es decir, transacciones no fradulentas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfica de barras con seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['Class']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinar la cantidad de muestras normales y fraudulentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Casos normales:',len(df[df['Class'] == 0]))\n",
    "print('Casos de fraude:',len(df[df['Class'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#También se puede utilizar la función value_counts()\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proporción de transacciones normales y fradulentas\n",
    "df['Class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Análisis de exploratorio de datos (EDA)\n",
    "### 5.1 Analizar el rasgo `Time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=df['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir el tiempo de segundos a horas para facilitar la interpretación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=t/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es la hora de la última transacción, en días?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.max()/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un histograma de las transacciones durante las 48 horas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=60)\n",
    "sns.distplot(t, bins=48, kde=True);\n",
    "plt.xlim([0,48])\n",
    "plt.xticks(np.arange(0,49,6))\n",
    "plt.xlabel('Horas')\n",
    "plt.ylabel('Número de transacciones')\n",
    "plt.title('Tiempos de transacciones');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el tiempo entre transacciones (segundos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdif=df['Time']-df['Time'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=60)\n",
    "plt.plot(tdif);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Nota</b>: Observar que hay dos lapsos con duraciones muy largas\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,4), dpi=60)\n",
    "ax1=fig.add_subplot(111, label=\"1\")\n",
    "ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "ax3=fig.add_subplot(111, label=\"3\", frame_on=False)\n",
    "ax1.hist(t,bins=48);\n",
    "ax2.plot(tdif,color='black', alpha=0.6);\n",
    "ax3.plot(t,color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,4), dpi=60)\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(tdif, linewidth=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Nota</b>: Observar outliers que muestra el diagrama de caja y bigotes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar el tiempo entre transacciones de acuerdo a la clase, pero primero agregar el rasgo del tiempo entre transacciones al dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_Difference'] = df['Time']-df['Time'].shift()\n",
    "df.groupby('Class').Time_Difference.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar gráficamente lo anterior mediante diagrama de caja y bigotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Class\", y=\"Time_Difference\",data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Se observa que el rango intercuartil (IQR) de ambas clases es similar; además, los valores atípicos de las diferencias de tiempo ocurren tanto en transacciones legítimas como fraudulentas. Sin embargo, algunos valores atípicos pueden indicar fraude, ya que el fraude a menudo ocurre en momentos en que hay pocas transacciones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.\n",
    "1. [Two (or more) graphs in one plot with different x-axis AND y-axis scales in python](https://stackoverflow.com/questions/42734109/two-or-more-graphs-in-one-plot-with-different-x-axis-and-y-axis-scales-in-pyth)\n",
    "1. [Gráficos con eje X común pero eje Y diferente: usando twinx](https://riptutorial.com/es/python/example/31794/graficos-con-eje-x-comun-pero-eje-y-diferente--usando-twinx---)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Analizar el rasgo `Amount`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumen de estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede verificar el 75% de las transacciones están son con cantidades menores a $77.00. La siguiente figura muestra el histograma de los montos de las transacciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.distplot(df['Amount'], bins=300, kde=False)\n",
    "plt.ylabel('Cantidad')\n",
    "plt.title('Monto de la transacción');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El histograma es difícil de leer debido a algunos valores atípicos que no podemos ver. Un diagrama de caja y bigotes mostrará los valores atípicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.boxplot(df['Amount'])\n",
    "plt.title('Monto de la transacción');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que no hay valores atípicos a la izquierda y muchos valores atípicos a la derecha. Por lo tanto, las cantidades están muy sesgadas a la derecha. Para asegurar se puede calcular la asimetría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un sesgo grande hacia la derecha. Hay que usar una transformación de potencia para aproximar los montos de las transacciones a una distribución normal. Se usará la transformación Box-Cox en SciPy, pero algunas de las cantidades son cero, así que primero hay que cambiar las cantidades para que sean positivas. Se cambiará por una cantidad muy pequeña de $10^{-9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número de ejemplos con Amount = 0\n",
    "(df['Amount'] == 0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumar 1^-9 a la columna Amount de todas las muestras\n",
    "df.loc[:,'Amount'] = df['Amount'] + 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Realizar la transformación Box-Cox al rasgo `Amount`: <br>\n",
    "> 1. https://es.wikipedia.org/wiki/Transformación_Box-Cox\n",
    "> 1. https://www.youtube.com/watch?v=s0XUDc_1tLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antes de la gausianización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(df['Amount'], line='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Amount'], bins=48, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'].kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gausianización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'Amount'], maxlog, (min_ci, max_ci) = sp.stats.boxcox(df['Amount'], alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la gausianización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(df['Amount'], line='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.distplot(df['Amount'], bins=48, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'].kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho mejor. La distribución parece ser bimodal, lo que sugiere una división entre compras \"pequeñas\" y \"grandes\". De esta forma, la transformación de potencia eliminó la mayor parte de la asimetría de la variable `Amount`. Ahora veamos las estadísticas descriptivas de las cantidades transformadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.boxplot(df['Amount'])\n",
    "plt.title('Monto de la transacción');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 `Time` vs `Amount` <br>\n",
    "¿Existe una relación entre los montos de la transacción y la hora del día? Crear un histograma conjunto de cajas hexagonales. Para este gráfico, se convierte cada tiempo de transacción a la hora del día en que ocurrió."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(t.apply(lambda x: x % 24), df['Amount'], kind='hex', stat_func=None, height=12, xlim=(0,24), ylim=(-7.5,14)).set_axis_labels('Hora del día (hr)','Monto transformado');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los montos de las transacciones parecen estar distribuidos de manera similar a lo largo del día. Sin embargo, en las primeras horas del día, alrededor de las 5-7 AM, los montos de alrededor de 2.5 son las más comunes (recuerde que este es un valor transformado de Box-Cox). Quizás las personas están comprando su café de la mañana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 `V1` vs `V28` \n",
    "Comparar las estadísticas descriptivas de los rasgos `V1`-`V28` transformados con PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vars = ['V%i' % k for k in range(1,29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabla completa de estadísticas descriptivas:\n",
    "df[pca_vars].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es complicado interpretar esta tabla, así que vamos a hacer algunas visualizaciones. Empezaremos por graficar las medias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.barplot(x=pca_vars, y=df[pca_vars].mean(), color='darkblue')\n",
    "plt.xlabel('Rasgo')\n",
    "plt.ylabel('Media')\n",
    "plt.title('Medias V1-V28');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los `V1`-`V28` tienen aproximadamente una media cero. Ahora graficar las desviaciones estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.barplot(x=pca_vars, y=df[pca_vars].std(), color='darkred')\n",
    "plt.xlabel('Rasgo')\n",
    "plt.ylabel('Deviación estándar')\n",
    "plt.title('Desviación estándar V1-V28');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las componentes principales tienen aproximadamente una varianza unitaria, que oscila entre ~0.3 y ~1.9. A continuación, se grafican las asimetrías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.barplot(x=pca_vars, y=df[pca_vars].skew(), color='darkgreen')\n",
    "plt.xlabel('Rasgo')\n",
    "plt.ylabel('Asimetría')\n",
    "plt.title('Asimetría (skewnesses) V1-V28 ');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de las PC están significativamente sesgadas. Graficar un histograma de una de las variables sesgadas, por ej. `V8`, para ver la distribución en detalle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.distplot(df['V8'], bins=300, kde=False)\n",
    "plt.ylabel('Magnitud')\n",
    "plt.title('V8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El histograma no muestra valores atípicos. Probar con un diagram de caja y bigotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.boxplot(df['V8'])\n",
    "plt.title('V8');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "El diagrama de caja también es difícil de interpretar debido al gran número de valores atípicos, lo que indica una alta curtosis en V8. Esto sugiere graficar las curtosis de las PC. El método de kurtosis empleado en Pandas es la definición de Fisher, donde la distribución normal estándar tiene kurtosis 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observar la escala logarítmica en el eje $y$ en la gráfica siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "plt.yscale('log')\n",
    "sns.barplot(x=pca_vars, y=df[pca_vars].kurtosis(), color='darkorange')\n",
    "plt.xlabel('Rasgo')\n",
    "plt.ylabel('Asimetría')\n",
    "plt.title('Asimetría (skewnesses) V1-V28 ');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha descubierto que muchas de las PC son de cola pesada (heavy-tailed). El gran número de valores atípicos en `V1`-`V28` hace necesario considerar estadísticas descriptivas robustas. Graficar las medianas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.barplot(x=pca_vars, y=df[pca_vars].median(), color='darkblue')\n",
    "plt.xlabel('Rasgo')\n",
    "plt.ylabel('Mediana')\n",
    "plt.title('Mediana de V1-V28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las medianas también son muy cercanas a cero. A continuación, analicemos los rangos intercuartiles (IQR)*:\n",
    "* Pandas no tiene un método IQR incorporado, pero se puede usar el método de cuantiles para calcular el IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "sns.barplot(x=pca_vars, y=df[pca_vars].quantile(0.75) - df[pca_vars].quantile(0.25), color='darkred')\n",
    "plt.xlabel('Rasgo')\n",
    "plt.ylabel('IQR')\n",
    "plt.title('IQRs V1-V28');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Matriz de correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero eliminar rasgo adicional\n",
    "df=df.drop(['Time_Difference'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corrmat = df.corr()\n",
    "fig = plt.figure(figsize = (12, 9))\n",
    "sns.heatmap(corrmat, cmap='seismic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar los rasgos gausianizados que presentan la mayor correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelado\n",
    "Ahora estamos listos para crear modelos de aprendizaje automático para predecir si una transacción es fraudulenta.<br> Entrenaremos a los siguientes modelos:\n",
    "1. Regresión logística\n",
    "1. Clasificador de soporte vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Clasificación con datos desbalanceados\n",
    "Utilizar la regresión logística para hacer la clasificación de estos datos desbalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Preparación de datos\n",
    "Antes de ejecutar el algoritmo, normalizar el rasgo `Amount` y eliminar el rasgo `Time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['normAmount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df=df.drop(['Time','Amount'],axis=1)\n",
    "X=df.drop(['Class'],axis=1)\n",
    "y=df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Notas sobre el uso de StandardScaler\n",
    "__`StandardScaler`__ : Transforma los datos de tal manera que su media sea 0 y su desviación estándar 1. En resumen, estandariza los datos. La estandarización es útil para datos que tienen valores negativos. Ajusta los datos a una distribución normal. Es más útil en la clasificación que en la regresión. \n",
    "StandardScaler estandariza un rasgo restando la media y luego escalándolo a una varianza unitaria. La desviación unitaria significa dividir todos los valores por la desviación estándar.<br><br>\n",
    "__`Normalizer`__ : Comprime los datos entre 0 y 1, esto es, efectua una normalización. Debido a la disminución del rango y la magnitud, los gradientes en el proceso de entrenamiento no se desbordarán y no se obtendrán valores más altos de pérdida. Es más útil en la regresión que en la clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Particionar dataset (entrenamiento y prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Aplicar el algoritmo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score,accuracy_score,precision_score,f1_score,confusion_matrix\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.4 Evaluación del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_test,y_pred)\n",
    "print('Matriz de confusión:')\n",
    "print(conf)\n",
    "print('Exactitud =',accuracy_score(y_test,y_pred))\n",
    "print('Sensibilidad [clase 0 , clase 1] =',recall_score(y_test,y_pred,average=None))\n",
    "print ('Precision =', precision_score(y_test, y_pred, average=None))\n",
    "print ('F1 =', f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.5 Observaciones:\n",
    "1. __Exactitud__: Podría creerse que el algoritmo está funcionando extremadamente bien. Pero no es verdad. Debido a que la mayoría de las etiquetas son 0, incluso con predicciones aleatorias dan una exactitud del 99%. Por lo tanto, se necesita una mejor métrica para entender el rendimiento del modelo.\n",
    "1. __Sensibilidad__: Como se puede observar en los resultados, la sensibilidad de 1 es sólo 0.61904762 comparado con el 99% de 0. Así que el modelo no está haciendo un buen trabajo en el reconocimiento de fraudes. Esto muestra cómo los datos desbalanceados están afectando la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La función classification_report nos proporciona las métricas anteriores\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pd.DataFrame(conf, range(2),range(2))\n",
    "fig = plt.figure(figsize = (3, 3))\n",
    "sns.heatmap(conf, annot=True, fmt='g', annot_kws={\"size\": 14}, cmap='seismic', cbar=False)\n",
    "plt.title('Matriz de confusión');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.6 validación cruzada de K iteraciones (K-fold cross-validation)\n",
    "\n",
    "Para evaluar el rendimiento del modelo de regresión logística para todo el conjunto de datos y no sólo para el conjunto de test (p. ej., el 30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "metrica = 'precision'\n",
    "results = model_selection.cross_val_score(logreg, X_train, y_train, cv=kfold, scoring=metrica)\n",
    "print('Valores de la métrica en cada iteración: ',results)\n",
    "print(\"Validación cruzada de 10 iteraciones, promedio de la métrica: %.3f\" % (results.mean()))\n",
    "print(\"Validación cruzada de 10 iteraciones, desviación estándard de la métrica: %.3f\" % (results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.7 Curvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "# Calcular TFP, TVP, thresholds y roc auc\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot([0,1],[0,1],'k--', label='Clasificador aleatorio')\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "plt.plot(fpr, tpr,'r-', label='Regresión logística')\n",
    "plt.plot([0,0,1],[0,1,1],'g-',label='Clasificador ideal')\n",
    "plt.title('Curva ROC')\n",
    "plt.xlabel('Tasa de falsos positivos')\n",
    "plt.ylabel('Tasa de verdaderos positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print('AUC =',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,average_precision_score\n",
    "lr_precision, lr_recall,_ = precision_recall_curve(y_test, y_pred_prob)\n",
    "plt.plot(lr_recall, lr_precision, label='Regresión logística')\n",
    "plt.plot([1,1,0],[0,1,1],'g-',label='Clasificador ideal')\n",
    "plt.title('Curva PR')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision');\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "print('AUC =',average_precision_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Clasificación con datos balanceados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Sobremuestreo de datos\n",
    "Se hará un sobremuestreo a la clase minoritaria con SMOTE (**_Synthetic Minority Over-sampling Technique_**), funciona buscando dos vecinos cercanos en una clase minoritaria, produciendo un nuevo punto intermedio entre los dos puntos existentes y añadiendo ese nuevo punto a la muestra (rasgos continuos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=4001,kind='regular')\n",
    "X_train_sob, y_train_sob = sm.fit_sample(X_train, y_train)\n",
    "pd.value_counts(y_train_sob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Nota: el algoritmo <b>SMOTE</b> ha sobremuestreado las instancias de las minorías y las ha hecho iguales a las de la clase mayoritaria. Ambas categorías tienen la misma cantidad de registros. Concretamente, la clase minoritaria se ha incrementado hasta el número total de clases mayoritarias.\n",
    "Ahora calcular la precisión y sensibilidad después de aplicar el algoritmo SMOTE (sobremuestreo).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Predicción y evaluación - Conjunto remuestreado\n",
    "A continuación se usará la regresión logística para probar el rendimiento en el mismo conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logreg_sob = LogisticRegression(solver='lbfgs',random_state=5001)\n",
    "from sklearn import linear_model\n",
    "logreg_sob = linear_model.LogisticRegression()\n",
    "logreg_sob.fit(X_train_sob,y_train_sob)\n",
    "y_pred_sob = logreg_sob.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir el reporte de clasificación \n",
    "#print(classification_report(y_test, y_pred_sob)) \n",
    "recall_score(y_test, y_pred_sob, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha reducido la exactitud al 98% en comparación con el modelo anterior, pero el valor de sensibilidad de la clase minoritaria ha mejorado al 92%. Este es un buen modelo comparado con el anterior. La sensibilidad es excelente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_sob = logreg_sob.predict_proba(X_test)[:,1]\n",
    "fpr_res, tpr_res, thresholds_res = roc_curve(y_test, y_pred_prob_sob)\n",
    "plt.plot(fpr_res, tpr_res,'r-', label='Regresión logística')\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob_sob)\n",
    "plt.show()\n",
    "print('AUC =',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_precision_sob, lr_recall_sob,_ = precision_recall_curve(y_test, y_pred_prob_sob)\n",
    "plt.plot(lr_recall_sob, lr_precision_sob, label='Logistic')\n",
    "plt.show()\n",
    "print('AUC =',average_precision_score(y_test, y_pred_prob_sob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Submuestreo de datos\n",
    "El algoritmo `NearMiss` es una técnica de submuestreo que se basa en el clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "X_train_sub, y_train_sub = nr.fit_sample(X_train, y_train) \n",
    "pd.value_counts(y_train_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "logreg2 = LogisticRegression(solver='lbfgs',random_state=1)\n",
    "logreg2.fit(X_train_sub, y_train_sub) \n",
    "y_pred_sub = logreg2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimir el reporte de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_sub)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Usando `Class Weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_balanced = LogisticRegression(solver='lbfgs',class_weight = 'balanced')\n",
    "lr_balanced.fit(X_train,y_train)\n",
    "y_balanced_pred = lr_balanced.predict(X_test)\n",
    "print(recall_score(y_test,y_balanced_pred))\n",
    "print(accuracy_score(y_test,y_balanced_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Selección del modelo\n",
    "### 7.1 Grid Search\n",
    "Mediante esta técnica se puede encontrar la mejor combinación de hiperparámetros de un algoritmo con el propósito de reducir el sobreajuste.\n",
    "Se basa en una búsqueda exhaustiva por el paradigma de fuerza bruta en el que se especifica una lista de valores para diferentes hiperparámetros y el método evalúa el rendimiento del modelo para cada combinación de éstos parámetros para obtener el conjunto óptimo que brinda el mayor rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar la clase GridSearchCV de la librería sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente, crear un diccionario de los parámetros y sus valores que se desea probar para obtener el mejor rendimiento. \n",
    "Los detalles de todos los parámetros para el algoritmo de bosque aleatorio están disponibles en los documentos de [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param={'C':np.logspace(-3,3,7), 'penalty':[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "#grid_param={'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "#grid_param={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir la búsqueda de rejilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv=GridSearchCV(logreg_sob,grid_param,cv=5, scoring='recall',verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer el ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = logreg_cv.fit(X_train_sob,y_train_sob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimir los valores de los hiperparámetros del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mejor penalización:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Mejor C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mejores hiperparámetros: \",logreg_cv.best_params_)\n",
    "print(\"Sensibilidad :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Utilizar los hiperparámetros obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_sob = linear_model.LogisticRegression(C=100,penalty='l2')\n",
    "logreg_sob.fit(X_train_sob,y_train_sob)\n",
    "y_pred_sob = logreg_sob.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir el reporte de clasificación \n",
    "print(classification_report(y_test, y_pred_sob)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Sensibilidad [0/1] = ',recall_score(y_test, y_pred_sob, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
